{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fidelity Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script trains the convolution neural network to recognize $\\mathcal{F}$ values for every number of bases employed in compressive tomography, and later predicts the values with test examples. The complete simulated data consist of those from both statistically noiseless and noisy cases of ACT and random Haar schemes (4 batches). Each batch consists of $m=5000$ training datasets generated from a separate MATLAB script that simulates the ACT and random Haar schemes on 4-qubit systems ($d=2^4=16$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spi\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "from keras.layers import Concatenate, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, merge, Input, ZeroPadding2D, Activation, Add, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import optimizers, regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines work parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=16 # Hilbert-space dimension\n",
    "m=8000 # Number of training datasets\n",
    "num_classes=1 # Set to 1 by default since we are fitting a single continuous output value\n",
    "\n",
    "# Defines the working directory containing all generated MATLAB MAT training data files\n",
    "work_dir='../training/'+str(m)+'_perf_noisy_ex/'\n",
    "\n",
    "# Defines ICCNet model storage directory\n",
    "stor_dir=work_dir+'FidNet_trained_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training commences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loops over different number of bases \"Kbas\" for training FidNet\n",
    "\n",
    "for l in np.linspace(1,10,10): # Defines the number of bases \"Kbas\" for training the ConvNet.\n",
    "\n",
    "    Kbas=int(l)\n",
    "\n",
    "    # Loads the relevant input matrix for the defined Kbas.\n",
    "    X = spi.loadmat(work_dir+'X_'+str(Kbas)+'.mat')['X']\n",
    "\n",
    "    # Loads the fidelity values.\n",
    "    y = spi.loadmat(work_dir+'y_'+str(Kbas)+'.mat')['y']\n",
    "    y = y[:,1].reshape([m,1])\n",
    "    y = np.abs(y)\n",
    "\n",
    "    if os.path.isdir(stor_dir)==False:\n",
    "        os.mkdir(stor_dir)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Data preprocessing and splitting to 'train', 'validation' and 'test' sets.\n",
    "    # --------------------------------------------------------------------------  \n",
    "\n",
    "    # Splits the full dataset into training, validation and test subsets.\n",
    "    x_train0, x_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_rest, y_rest, test_size=0.5, shuffle=True)\n",
    "\n",
    "    # Performs standard data normalization and scaling.\n",
    "    x_train = preprocessing.StandardScaler().fit(x_train0).transform(x_train0)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = preprocessing.StandardScaler().fit(x_train0).transform(x_test)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_val = preprocessing.StandardScaler().fit(x_train0).transform(x_val)\n",
    "    x_val = x_val.astype('float32')  \n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # FidNet model definition and parametrization.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Defines gradient optimization batch size and training epochs.\n",
    "    batch_size = 1024\n",
    "    epochs = 500\n",
    "    \n",
    "    tf.keras.initializers.he_normal()\n",
    "\n",
    "    # Converts inputs to single-channel 2D images.\n",
    "    img_rows = int(np.ceil(np.power(x_train.shape[1],0.5)))\n",
    "    img_cols = img_rows\n",
    "    x_train = np.concatenate((x_train,np.zeros((x_train.shape[0],img_rows*img_cols-x_train.shape[1]))),axis=1)\n",
    "    x_test = np.concatenate((x_test,np.zeros((x_test.shape[0],img_rows*img_cols-x_test.shape[1]))),axis=1)\n",
    "    x_val = np.concatenate((x_val,np.zeros((x_val.shape[0],img_rows*img_cols-x_val.shape[1]))),axis=1)\n",
    "\n",
    "    K.clear_session()\n",
    "    x_train=x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test=x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    x_val=x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1) \n",
    "    \n",
    "    # Sets up neural network\n",
    "    X_input = Input(input_shape) \n",
    "\n",
    "    Xnn = Conv2D(4, kernel_size = (5,5), padding='valid',strides=1, kernel_initializer = 'he_uniform')(X_input)\n",
    "    Xnn = BatchNormalization()(Xnn)\n",
    "    Xnn = Activation('relu')(Xnn)\n",
    "    Xnn = Dropout(0.3)(Xnn)\n",
    "\n",
    "    Xnn = Conv2D(8, kernel_size = (5,5), padding='valid',strides=1, kernel_initializer = 'he_uniform')(Xnn)\n",
    "    Xnn = BatchNormalization()(Xnn)\n",
    "    Xnn = Activation('relu')(Xnn)\n",
    "    Xnn = Dropout(0.5)(Xnn)\n",
    "\n",
    "    Xnn = Conv2D(16, kernel_size = (5,5), padding='valid',strides=1, kernel_initializer = 'he_uniform')(Xnn)\n",
    "    Xnn = BatchNormalization()(Xnn)\n",
    "    Xnn = Activation('relu')(Xnn)\n",
    "    Xnn = Dropout(0.5)(Xnn)\n",
    "\n",
    "    Xnn = AveragePooling2D(pool_size=(2,2),strides=2)(Xnn)\n",
    "\n",
    "    Xnn = Flatten()(Xnn)\n",
    "\n",
    "    Xnn = Dense(32)(Xnn)\n",
    "    Xnn = BatchNormalization()(Xnn)\n",
    "    Xnn = Activation('sigmoid')(Xnn)\n",
    "    Xnn = Dropout(0.5)(Xnn)\n",
    "\n",
    "    Xnn = Dense(num_classes,activation='sigmoid')(Xnn)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = Xnn)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='mse',optimizer='nadam')\n",
    "\n",
    "    # -----------------\n",
    "    # Network training.\n",
    "    # -----------------\n",
    "\n",
    "    # Sets up the scoreboard and checkpoint monitor to record training results.    \n",
    "    checkpointer = ModelCheckpoint(filepath=stor_dir+\"model_FidNet_K_\"+str(Kbas)+\".h5\",\n",
    "                                   verbose=0, monitor='val_loss', mode='min', save_best_only=True)\n",
    "    tensorboard = TensorBoard(log_dir='./logs',\n",
    "                              histogram_freq=0,\n",
    "                              write_graph=True,\n",
    "                              write_images=True)\n",
    "\n",
    "    # Trains ICCNet with training and validation data subsets and records all results.\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "    with open(stor_dir+\"model_FidNet_K_\"+str(Kbas)+\"_hist.txt\", 'w') as outfile:  \n",
    "        json.dump(history, outfile)\n",
    "\n",
    "    # Evaluates the trained model with test data subsets.\n",
    "    score=model.evaluate(x_test,y_test,verbose=0)\n",
    "\n",
    "    print('Test loss:',score)\n",
    "\n",
    "    # Stores all split data subsets into a dictionary and saves it into a MATLAB cell file.\n",
    "    Xy_dict = {'x_val': x_val, 'y_val': y_val, 'x_test': x_test, 'y_test': y_test, 'x_train0': x_train0, 'y_train': y_train}\n",
    "    spi.savemat(stor_dir+\"model_FidNet_K_\"+str(Kbas)+\"_Xy.mat\", Xy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysteo-tensorflow_old",
   "language": "python",
   "name": "ysteo-tensorflow_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
